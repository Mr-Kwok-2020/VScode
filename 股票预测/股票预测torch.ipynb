{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "import os\n",
    "import warnings\n",
    "# 禁用警告\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device=\"cuda\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import GroupKFold\n",
    "NUM_WORKERS = 4\n",
    "from pandas_datareader.data import DataReader\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "yf.pdr_override()\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# The tech stocks we'll use for this analysis\n",
    "tech_list = ['AAPL', 'GOOG', 'MSFT', 'AMZN']\n",
    "end = datetime.now()\n",
    "start = datetime(end.year -3, end.month-0, end.day)\n",
    "\n",
    "for stock in tech_list:\n",
    "    globals()[stock] = yf.download(stock, start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data = [AAPL, GOOG, MSFT, AMZN]\n",
    "company_name = [\"APPLE\", \"GOOGLE\", \"MICROSOFT\", \"AMAZON\"]\n",
    "for company_data, com_name in zip(company_data, company_name):\n",
    "    company_data[\"company_name\"] = com_name\n",
    "    \n",
    "# df = pd.concat(company_list, axis=0)\n",
    "# df.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(755, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(755, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AAPL_Close_df = AAPL.filter(['Close'])\n",
    "print(type(AAPL_Close_df))\n",
    "print(AAPL_Close_df.shape)\n",
    "AAPL_Close =AAPL_Close_df.values\n",
    "print(type(AAPL_Close))\n",
    "print(AAPL_Close.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(755, 1)\n"
     ]
    }
   ],
   "source": [
    "# 归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "AAPL_Close = scaler.fit_transform(AAPL_Close)\n",
    "\n",
    "AAPL_Close\n",
    "print(AAPL_Close.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(10, 1) [0.10405764]\n",
      "11\n",
      "(10, 1) [0.08824552]\n",
      "12\n",
      "(10, 1) [0.09533772]\n",
      "13\n",
      "(10, 1) [0.08022316]\n",
      "14\n",
      "(10, 1) [0.03964651]\n",
      "15\n",
      "(10, 1) [0.05499356]\n",
      "16\n",
      "(10, 1) [0.0649924]\n",
      "17\n",
      "(10, 1) [0.07150325]\n",
      "18\n",
      "(10, 1) [0.10010465]\n",
      "19\n",
      "(10, 1) [0.14277409]\n",
      "20\n",
      "(10, 1) [0.14695966]\n",
      "21\n",
      "(10, 1) [0.14533195]\n",
      "22\n",
      "(10, 1) [0.1373096]\n",
      "23\n",
      "(10, 1) [0.15474943]\n",
      "24\n",
      "(10, 1) [0.16207413]\n",
      "25\n",
      "(10, 1) [0.1318451]\n",
      "26\n",
      "(10, 1) [0.14881986]\n",
      "27\n",
      "(10, 1) [0.13916989]\n",
      "28\n",
      "(10, 1) [0.1318451]\n",
      "29\n",
      "(10, 1) [0.20276707]\n",
      "30\n",
      "(10, 1) [0.20195322]\n",
      "31\n",
      "(10, 1) [0.21230084]\n",
      "32\n",
      "(10, 1) [0.18858275]\n",
      "33\n",
      "(10, 1) [0.20683635]\n",
      "34\n",
      "(10, 1) [0.24927338]\n",
      "35\n",
      "(10, 1) [0.23857697]\n",
      "36\n",
      "(10, 1) [0.25031973]\n",
      "37\n",
      "(10, 1) [0.30519709]\n",
      "38\n",
      "(10, 1) [0.28403667]\n",
      "39\n",
      "(10, 1) [0.2706662]\n",
      "40\n",
      "(10, 1) [0.25869087]\n",
      "41\n",
      "(10, 1) [0.22055578]\n",
      "42\n",
      "(10, 1) [0.23915816]\n",
      "43\n",
      "(10, 1) [0.1878851]\n",
      "44\n",
      "(10, 1) [0.23811181]\n",
      "45\n",
      "(10, 1) [0.25124988]\n",
      "46\n",
      "(10, 1) [0.21555627]\n",
      "47\n",
      "(10, 1) [0.21346357]\n",
      "48\n",
      "(10, 1) [0.23776303]\n",
      "49\n",
      "(10, 1) [0.2147425]\n",
      "50\n",
      "(10, 1) [0.19416345]\n",
      "51\n",
      "(10, 1) [0.2021858]\n",
      "52\n",
      "(10, 1) [0.2510173]\n",
      "53\n",
      "(10, 1) [0.30728978]\n",
      "54\n",
      "(10, 1) [0.33286835]\n",
      "55\n",
      "(10, 1) [0.37763048]\n",
      "56\n",
      "(10, 1) [0.38042092]\n",
      "57\n",
      "(10, 1) [0.36763164]\n",
      "58\n",
      "(10, 1) [0.30984764]\n",
      "59\n",
      "(10, 1) [0.25020353]\n",
      "60\n",
      "(10, 1) [0.27554934]\n",
      "61\n",
      "(10, 1) [0.28543198]\n",
      "62\n",
      "(10, 1) [0.27322406]\n",
      "63\n",
      "(10, 1) [0.31333564]\n",
      "64\n",
      "(10, 1) [0.30601086]\n",
      "65\n",
      "(10, 1) [0.30775495]\n",
      "66\n",
      "(10, 1) [0.29729094]\n",
      "67\n",
      "(10, 1) [0.29008253]\n",
      "68\n",
      "(10, 1) [0.28705969]\n",
      "69\n",
      "(10, 1) [0.28984995]\n",
      "70\n",
      "(10, 1) [0.26450414]\n",
      "71\n",
      "(10, 1) [0.23718167]\n",
      "72\n",
      "(10, 1) [0.22404378]\n",
      "73\n",
      "(10, 1) [0.22590389]\n",
      "74\n",
      "(10, 1) [0.18090918]\n",
      "75\n",
      "(10, 1) [0.17928147]\n",
      "76\n",
      "(10, 1) [0.1733519]\n",
      "77\n",
      "(10, 1) [0.12266011]\n",
      "78\n",
      "(10, 1) [0.12579933]\n",
      "79\n",
      "(10, 1) [0.20172073]\n",
      "80\n",
      "(10, 1) [0.17067784]\n",
      "81\n",
      "(10, 1) [0.13510052]\n",
      "82\n",
      "(10, 1) [0.11266127]\n",
      "83\n",
      "(10, 1) [0.12765953]\n",
      "84\n",
      "(10, 1) [0.06882919]\n",
      "85\n",
      "(10, 1) [0.12382275]\n",
      "86\n",
      "(10, 1) [0.11091735]\n",
      "87\n",
      "(10, 1) [0.13393788]\n",
      "88\n",
      "(10, 1) [0.12312518]\n",
      "89\n",
      "(10, 1) [0.15753978]\n",
      "90\n",
      "(10, 1) [0.17590976]\n",
      "91\n",
      "(10, 1) [0.16649228]\n",
      "92\n",
      "(10, 1) [0.11731191]\n",
      "93\n",
      "(10, 1) [0.11103355]\n",
      "94\n",
      "(10, 1) [0.15056386]\n",
      "95\n",
      "(10, 1) [0.14068131]\n",
      "96\n",
      "(10, 1) [0.11219619]\n",
      "97\n",
      "(10, 1) [0.11800947]\n",
      "98\n",
      "(10, 1) [0.12521797]\n",
      "99\n",
      "(10, 1) [0.12731075]\n",
      "100\n",
      "(10, 1) [0.10998721]\n",
      "101\n",
      "(10, 1) [0.13614696]\n",
      "102\n",
      "(10, 1) [0.14602951]\n",
      "103\n",
      "(10, 1) [0.17974654]\n",
      "104\n",
      "(10, 1) [0.18335075]\n",
      "105\n",
      "(10, 1) [0.20299965]\n",
      "106\n",
      "(10, 1) [0.23160097]\n",
      "107\n",
      "(10, 1) [0.26229507]\n",
      "108\n",
      "(10, 1) [0.2418324]\n",
      "109\n",
      "(10, 1) [0.27892096]\n",
      "110\n",
      "(10, 1) [0.2510173]\n",
      "111\n",
      "(10, 1) [0.2797349]\n",
      "112\n",
      "(10, 1) [0.27578192]\n",
      "113\n",
      "(10, 1) [0.28368789]\n",
      "114\n",
      "(10, 1) [0.263574]\n",
      "115\n",
      "(10, 1) [0.26810835]\n",
      "116\n",
      "(10, 1) [0.24997095]\n",
      "117\n",
      "(10, 1) [0.27764221]\n",
      "118\n",
      "(10, 1) [0.28229276]\n",
      "119\n",
      "(10, 1) [0.27845598]\n",
      "120\n",
      "(10, 1) [0.26903849]\n",
      "121\n",
      "(10, 1) [0.26787577]\n",
      "122\n",
      "(10, 1) [0.24439025]\n",
      "123\n",
      "(10, 1) [0.25694678]\n",
      "124\n",
      "(10, 1) [0.20241829]\n",
      "125\n",
      "(10, 1) [0.20532502]\n",
      "126\n",
      "(10, 1) [0.22439256]\n",
      "127\n",
      "(10, 1) [0.22985706]\n",
      "128\n",
      "(10, 1) [0.19079173]\n",
      "129\n",
      "(10, 1) [0.17986283]\n",
      "130\n",
      "(10, 1) [0.14335537]\n",
      "131\n",
      "(10, 1) [0.16893384]\n",
      "132\n",
      "(10, 1) [0.19776765]\n",
      "133\n",
      "(10, 1) [0.18404831]\n",
      "134\n",
      "(10, 1) [0.16753862]\n",
      "135\n",
      "(10, 1) [0.16567842]\n",
      "136\n",
      "(10, 1) [0.19613994]\n",
      "137\n",
      "(10, 1) [0.17428205]\n",
      "138\n",
      "(10, 1) [0.19369837]\n",
      "139\n",
      "(10, 1) [0.1913731]\n",
      "140\n",
      "(10, 1) [0.19079173]\n",
      "141\n",
      "(10, 1) [0.17253805]\n",
      "142\n",
      "(10, 1) [0.16474827]\n",
      "143\n",
      "(10, 1) [0.16091149]\n",
      "144\n",
      "(10, 1) [0.16998019]\n",
      "145\n",
      "(10, 1) [0.15230786]\n",
      "146\n",
      "(10, 1) [0.17963025]\n",
      "147\n",
      "(10, 1) [0.17974654]\n",
      "148\n",
      "(10, 1) [0.18951281]\n",
      "149\n",
      "(10, 1) [0.19404716]\n",
      "150\n",
      "(10, 1) [0.18218811]\n",
      "151\n",
      "(10, 1) [0.19660501]\n",
      "152\n",
      "(10, 1) [0.2329961]\n",
      "153\n",
      "(10, 1) [0.22322984]\n",
      "154\n",
      "(10, 1) [0.22915932]\n",
      "155\n",
      "(10, 1) [0.24822686]\n",
      "156\n",
      "(10, 1) [0.2327637]\n",
      "157\n",
      "(10, 1) [0.25415652]\n",
      "158\n",
      "(10, 1) [0.27368905]\n",
      "159\n",
      "(10, 1) [0.27043362]\n",
      "160\n",
      "(10, 1) [0.267062]\n",
      "161\n",
      "(10, 1) [0.263574]\n",
      "162\n",
      "(10, 1) [0.28299033]\n",
      "163\n",
      "(10, 1) [0.30101152]\n",
      "164\n",
      "(10, 1) [0.30833631]\n",
      "165\n",
      "(10, 1) [0.31194051]\n",
      "166\n",
      "(10, 1) [0.34321598]\n",
      "167\n",
      "(10, 1) [0.36716665]\n",
      "168\n",
      "(10, 1) [0.39681441]\n",
      "169\n",
      "(10, 1) [0.38135107]\n",
      "170\n",
      "(10, 1) [0.40309267]\n",
      "171\n",
      "(10, 1) [0.39600046]\n",
      "172\n",
      "(10, 1) [0.40925473]\n",
      "173\n",
      "(10, 1) [0.45006388]\n",
      "174\n",
      "(10, 1) [0.4422741]\n",
      "175\n",
      "(10, 1) [0.41797465]\n",
      "176\n",
      "(10, 1) [0.37216599]\n",
      "177\n",
      "(10, 1) [0.41518421]\n",
      "178\n",
      "(10, 1) [0.40646429]\n",
      "179\n",
      "(10, 1) [0.42274158]\n",
      "180\n",
      "(10, 1) [0.44320425]\n",
      "181\n",
      "(10, 1) [0.44820376]\n",
      "182\n",
      "(10, 1) [0.42239279]\n",
      "183\n",
      "(10, 1) [0.40158116]\n",
      "184\n",
      "(10, 1) [0.40925473]\n",
      "185\n",
      "(10, 1) [0.41181258]\n",
      "186\n",
      "(10, 1) [0.4078596]\n",
      "187\n",
      "(10, 1) [0.42925242]\n",
      "188\n",
      "(10, 1) [0.42448549]\n",
      "189\n",
      "(10, 1) [0.42576442]\n",
      "190\n",
      "(10, 1) [0.41506801]\n",
      "191\n",
      "(10, 1) [0.41448664]\n",
      "192\n",
      "(10, 1) [0.40878974]\n",
      "193\n",
      "(10, 1) [0.41181258]\n",
      "194\n",
      "(10, 1) [0.44704103]\n",
      "195\n",
      "(10, 1) [0.44948269]\n",
      "196\n",
      "(10, 1) [0.4729682]\n",
      "197\n",
      "(10, 1) [0.46215559]\n",
      "198\n",
      "(10, 1) [0.41762586]\n",
      "199\n",
      "(10, 1) [0.42157885]\n",
      "200\n",
      "(10, 1) [0.43890248]\n",
      "201\n",
      "(10, 1) [0.4565749]\n",
      "202\n",
      "(10, 1) [0.45552837]\n",
      "203\n",
      "(10, 1) [0.44087897]\n",
      "204\n",
      "(10, 1) [0.43134511]\n",
      "205\n",
      "(10, 1) [0.44366941]\n",
      "206\n",
      "(10, 1) [0.49622132]\n",
      "207\n",
      "(10, 1) [0.48122314]\n",
      "208\n",
      "(10, 1) [0.48912911]\n",
      "209\n",
      "(10, 1) [0.50238338]\n",
      "210\n",
      "(10, 1) [0.50994074]\n",
      "211\n",
      "(10, 1) [0.53772821]\n",
      "212\n",
      "(10, 1) [0.51935823]\n",
      "213\n",
      "(10, 1) [0.50726669]\n",
      "214\n",
      "(10, 1) [0.44797118]\n",
      "215\n",
      "(10, 1) [0.4547146]\n",
      "216\n",
      "(10, 1) [0.43808854]\n",
      "217\n",
      "(10, 1) [0.44866875]\n",
      "218\n",
      "(10, 1) [0.44587831]\n",
      "219\n",
      "(10, 1) [0.41413786]\n",
      "220\n",
      "(10, 1) [0.37786306]\n",
      "221\n",
      "(10, 1) [0.38355996]\n",
      "222\n",
      "(10, 1) [0.41169638]\n",
      "223\n",
      "(10, 1) [0.42309036]\n",
      "224\n",
      "(10, 1) [0.42413671]\n",
      "225\n",
      "(10, 1) [0.40611551]\n",
      "226\n",
      "(10, 1) [0.36588773]\n",
      "227\n",
      "(10, 1) [0.37658414]\n",
      "228\n",
      "(10, 1) [0.36112079]\n",
      "229\n",
      "(10, 1) [0.37449126]\n",
      "230\n",
      "(10, 1) [0.33368212]\n",
      "231\n",
      "(10, 1) [0.35658644]\n",
      "232\n",
      "(10, 1) [0.36693407]\n",
      "233\n",
      "(10, 1) [0.38193225]\n",
      "234\n",
      "(10, 1) [0.3773979]\n",
      "235\n",
      "(10, 1) [0.37635155]\n",
      "236\n",
      "(10, 1) [0.361237]\n",
      "237\n",
      "(10, 1) [0.35426117]\n",
      "238\n",
      "(10, 1) [0.38739675]\n",
      "239\n",
      "(10, 1) [0.39995345]\n",
      "240\n",
      "(10, 1) [0.41983494]\n",
      "241\n",
      "(10, 1) [0.44552953]\n",
      "242\n",
      "(10, 1) [0.4513428]\n",
      "243\n",
      "(10, 1) [0.45390066]\n",
      "244\n",
      "(10, 1) [0.44471576]\n",
      "245\n",
      "(10, 1) [0.4441344]\n",
      "246\n",
      "(10, 1) [0.45204055]\n",
      "247\n",
      "(10, 1) [0.44657605]\n",
      "248\n",
      "(10, 1) [0.48982685]\n",
      "249\n",
      "(10, 1) [0.45762124]\n",
      "250\n",
      "(10, 1) [0.44785498]\n",
      "251\n",
      "(10, 1) [0.4601791]\n",
      "252\n",
      "(10, 1) [0.47727015]\n",
      "253\n",
      "(10, 1) [0.47110809]\n",
      "254\n",
      "(10, 1) [0.4748285]\n",
      "255\n",
      "(10, 1) [0.46506223]\n",
      "256\n",
      "(10, 1) [0.469364]\n",
      "257\n",
      "(10, 1) [0.43576326]\n",
      "258\n",
      "(10, 1) [0.4351819]\n",
      "259\n",
      "(10, 1) [0.45983032]\n",
      "260\n",
      "(10, 1) [0.45994652]\n",
      "261\n",
      "(10, 1) [0.47157307]\n",
      "262\n",
      "(10, 1) [0.50052326]\n",
      "263\n",
      "(10, 1) [0.55144746]\n",
      "264\n",
      "(10, 1) [0.58260672]\n",
      "265\n",
      "(10, 1) [0.58807121]\n",
      "266\n",
      "(10, 1) [0.59260556]\n",
      "267\n",
      "(10, 1) [0.59876762]\n",
      "268\n",
      "(10, 1) [0.53912334]\n",
      "269\n",
      "(10, 1) [0.57900251]\n",
      "270\n",
      "(10, 1) [0.63783286]\n",
      "271\n",
      "(10, 1) [0.6316708]\n",
      "272\n",
      "(10, 1) [0.61992786]\n",
      "273\n",
      "(10, 1) [0.5976049]\n",
      "274\n",
      "(10, 1) [0.63806544]\n",
      "275\n",
      "(10, 1) [0.70619689]\n",
      "276\n",
      "(10, 1) [0.75154056]\n",
      "277\n",
      "(10, 1) [0.7454947]\n",
      "278\n",
      "(10, 1) [0.80234855]\n",
      "279\n",
      "(10, 1) [0.75921413]\n",
      "280\n",
      "(10, 1) [0.74282064]\n",
      "281\n",
      "(10, 1) [0.80060464]\n",
      "282\n",
      "(10, 1) [0.71875359]\n",
      "283\n",
      "(10, 1) [0.7057319]\n",
      "284\n",
      "(10, 1) [0.689571]\n",
      "285\n",
      "(10, 1) [0.7272411]\n",
      "286\n",
      "(10, 1) [0.7580514]\n",
      "287\n",
      "(10, 1) [0.76549239]\n",
      "288\n",
      "(10, 1) [0.81257998]\n",
      "289\n",
      "(10, 1) [0.80048826]\n",
      "290\n",
      "(10, 1) [0.80153478]\n",
      "291\n",
      "(10, 1) [0.78781536]\n",
      "292\n",
      "(10, 1) [0.78049075]\n",
      "293\n",
      "(10, 1) [0.83211251]\n",
      "294\n",
      "(10, 1) [0.80525519]\n",
      "295\n",
      "(10, 1) [0.74968027]\n",
      "296\n",
      "(10, 1) [0.71573075]\n",
      "297\n",
      "(10, 1) [0.71770724]\n",
      "298\n",
      "(10, 1) [0.71793982]\n",
      "299\n",
      "(10, 1) [0.75154056]\n",
      "300\n",
      "(10, 1) [0.75677247]\n",
      "301\n",
      "(10, 1) [0.71793982]\n",
      "302\n",
      "(10, 1) [0.72817125]\n",
      "303\n",
      "(10, 1) [0.69015236]\n",
      "304\n",
      "(10, 1) [0.64864547]\n",
      "305\n",
      "(10, 1) [0.62864778]\n",
      "306\n",
      "(10, 1) [0.60423212]\n",
      "307\n",
      "(10, 1) [0.59504704]\n",
      "308\n",
      "(10, 1) [0.57365422]\n",
      "309\n",
      "(10, 1) [0.57260787]\n",
      "310\n",
      "(10, 1) [0.56714338]\n",
      "311\n",
      "(10, 1) [0.69631442]\n",
      "312\n",
      "(10, 1) [0.74805256]\n",
      "313\n",
      "(10, 1) [0.74607606]\n",
      "314\n",
      "(10, 1) [0.76037668]\n",
      "315\n",
      "(10, 1) [0.72619458]\n",
      "316\n",
      "(10, 1) [0.7202651]\n",
      "317\n",
      "(10, 1) [0.71177776]\n",
      "318\n",
      "(10, 1) [0.74863392]\n",
      "319\n",
      "(10, 1) [0.76549239]\n",
      "320\n",
      "(10, 1) [0.71712588]\n",
      "321\n",
      "(10, 1) [0.67666551]\n",
      "322\n",
      "(10, 1) [0.67945595]\n",
      "323\n",
      "(10, 1) [0.72491565]\n",
      "324\n",
      "(10, 1) [0.72212539]\n",
      "325\n",
      "(10, 1) [0.67945595]\n",
      "326\n",
      "(10, 1) [0.66108597]\n",
      "327\n",
      "(10, 1) [0.62643888]\n",
      "328\n",
      "(10, 1) [0.57702602]\n",
      "329\n",
      "(10, 1) [0.6080689]\n",
      "330\n",
      "(10, 1) [0.63260094]\n",
      "331\n",
      "(10, 1) [0.63573999]\n",
      "332\n",
      "(10, 1) [0.61341702]\n",
      "333\n",
      "(10, 1) [0.65248225]\n",
      "334\n",
      "(10, 1) [0.64864547]\n",
      "335\n",
      "(10, 1) [0.61306824]\n",
      "336\n",
      "(10, 1) [0.56807352]\n",
      "337\n",
      "(10, 1) [0.54644812]\n",
      "338\n",
      "(10, 1) [0.61051038]\n",
      "339\n",
      "(10, 1) [0.55900482]\n",
      "340\n",
      "(10, 1) [0.51494008]\n",
      "341\n",
      "(10, 1) [0.46715493]\n",
      "342\n",
      "(10, 1) [0.51912565]\n",
      "343\n",
      "(10, 1) [0.57144515]\n",
      "344\n",
      "(10, 1) [0.58342048]\n",
      "345\n",
      "(10, 1) [0.62248572]\n",
      "346\n",
      "(10, 1) [0.638763]\n",
      "347\n",
      "(10, 1) [0.67875838]\n",
      "348\n",
      "(10, 1) [0.69491929]\n",
      "349\n",
      "(10, 1) [0.7397978]\n",
      "350\n",
      "(10, 1) [0.74735499]\n",
      "351\n",
      "(10, 1) [0.75758642]\n",
      "352\n",
      "(10, 1) [0.79665165]\n",
      "353\n",
      "(10, 1) [0.78281602]\n",
      "354\n",
      "(10, 1) [0.74607606]\n",
      "355\n",
      "(10, 1) [0.74258806]\n",
      "356\n",
      "(10, 1) [0.79060579]\n",
      "357\n",
      "(10, 1) [0.75130798]\n",
      "358\n",
      "(10, 1) [0.71375425]\n",
      "359\n",
      "(10, 1) [0.71735846]\n",
      "360\n",
      "(10, 1) [0.69352398]\n",
      "361\n",
      "(10, 1) [0.64306477]\n",
      "362\n",
      "(10, 1) [0.66527154]\n",
      "363\n",
      "(10, 1) [0.69712819]\n",
      "364\n",
      "(10, 1) [0.63771648]\n",
      "365\n",
      "(10, 1) [0.6351588]\n",
      "366\n",
      "(10, 1) [0.66224852]\n",
      "367\n",
      "(10, 1) [0.66027203]\n",
      "368\n",
      "(10, 1) [0.65085454]\n",
      "369\n",
      "(10, 1) [0.59702353]\n",
      "370\n",
      "(10, 1) [0.60969661]\n",
      "371\n",
      "(10, 1) [0.53900713]\n",
      "372\n",
      "(10, 1) [0.53633308]\n",
      "373\n",
      "(10, 1) [0.61853273]\n",
      "374\n",
      "(10, 1) [0.5488896]\n",
      "375\n",
      "(10, 1) [0.55249398]\n",
      "376\n",
      "(10, 1) [0.57016622]\n",
      "377\n",
      "(10, 1) [0.64620399]\n",
      "378\n",
      "(10, 1) [0.53865835]\n",
      "379\n",
      "(10, 1) [0.54458783]\n",
      "380\n",
      "(10, 1) [0.4838972]\n",
      "381\n",
      "(10, 1) [0.51238222]\n",
      "382\n",
      "(10, 1) [0.41925357]\n",
      "383\n",
      "(10, 1) [0.37344492]\n",
      "384\n",
      "(10, 1) [0.42634578]\n",
      "385\n",
      "(10, 1) [0.408092]\n",
      "386\n",
      "(10, 1) [0.4511104]\n",
      "387\n",
      "(10, 1) [0.35321482]\n",
      "388\n",
      "(10, 1) [0.31287066]\n",
      "389\n",
      "(10, 1) [0.31566092]\n",
      "390\n",
      "(10, 1) [0.37983956]\n",
      "391\n",
      "(10, 1) [0.34786653]\n",
      "392\n",
      "(10, 1) [0.34972682]\n",
      "393\n",
      "(10, 1) [0.38762933]\n",
      "394\n",
      "(10, 1) [0.45576095]\n",
      "395\n",
      "(10, 1) [0.44645967]\n",
      "396\n",
      "(10, 1) [0.44494834]\n",
      "397\n",
      "(10, 1) [0.47401473]\n",
      "398\n",
      "(10, 1) [0.40623189]\n",
      "399\n",
      "(10, 1) [0.41506801]\n",
      "400\n",
      "(10, 1) [0.44494834]\n",
      "401\n",
      "(10, 1) [0.43622842]\n",
      "402\n",
      "(10, 1) [0.37437506]\n",
      "403\n",
      "(10, 1) [0.3103128]\n",
      "404\n",
      "(10, 1) [0.24927338]\n",
      "405\n",
      "(10, 1) [0.25950463]\n",
      "406\n",
      "(10, 1) [0.29054752]\n",
      "407\n",
      "(10, 1) [0.22811297]\n",
      "408\n",
      "(10, 1) [0.2455528]\n",
      "409\n",
      "(10, 1) [0.29566323]\n",
      "410\n",
      "(10, 1) [0.28961755]\n",
      "411\n",
      "(10, 1) [0.32356707]\n",
      "412\n",
      "(10, 1) [0.36298109]\n",
      "413\n",
      "(10, 1) [0.36298109]\n",
      "414\n",
      "(10, 1) [0.31391701]\n",
      "415\n",
      "(10, 1) [0.33472846]\n",
      "416\n",
      "(10, 1) [0.30554587]\n",
      "417\n",
      "(10, 1) [0.33124046]\n",
      "418\n",
      "(10, 1) [0.36181836]\n",
      "419\n",
      "(10, 1) [0.37763048]\n",
      "420\n",
      "(10, 1) [0.41750966]\n",
      "421\n",
      "(10, 1) [0.42553184]\n",
      "422\n",
      "(10, 1) [0.40030223]\n",
      "423\n",
      "(10, 1) [0.41181258]\n",
      "424\n",
      "(10, 1) [0.40751082]\n",
      "425\n",
      "(10, 1) [0.4421579]\n",
      "426\n",
      "(10, 1) [0.46192301]\n",
      "427\n",
      "(10, 1) [0.4258808]\n",
      "428\n",
      "(10, 1) [0.47157307]\n",
      "429\n",
      "(10, 1) [0.49529117]\n",
      "430\n",
      "(10, 1) [0.52214866]\n",
      "431\n",
      "(10, 1) [0.50749909]\n",
      "432\n",
      "(10, 1) [0.49424482]\n",
      "433\n",
      "(10, 1) [0.47854908]\n",
      "434\n",
      "(10, 1) [0.53889075]\n",
      "435\n",
      "(10, 1) [0.54540177]\n",
      "436\n",
      "(10, 1) [0.60539467]\n",
      "437\n",
      "(10, 1) [0.59376811]\n",
      "438\n",
      "(10, 1) [0.57632828]\n",
      "439\n",
      "(10, 1) [0.64748292]\n",
      "440\n",
      "(10, 1) [0.64376234]\n",
      "441\n",
      "(10, 1) [0.63841422]\n",
      "442\n",
      "(10, 1) [0.63283335]\n",
      "443\n",
      "(10, 1) [0.63341471]\n",
      "444\n",
      "(10, 1) [0.68364152]\n",
      "445\n",
      "(10, 1) [0.6749216]\n",
      "446\n",
      "(10, 1) [0.71689347]\n",
      "447\n",
      "(10, 1) [0.72956638]\n",
      "448\n",
      "(10, 1) [0.72770608]\n",
      "449\n",
      "(10, 1) [0.7453785]\n",
      "450\n",
      "(10, 1) [0.74072777]\n",
      "451\n",
      "(10, 1) [0.71015005]\n",
      "452\n",
      "(10, 1) [0.66422519]\n",
      "453\n",
      "(10, 1) [0.66027203]\n",
      "454\n",
      "(10, 1) [0.66376003]\n",
      "455\n",
      "(10, 1) [0.69282642]\n",
      "456\n",
      "(10, 1) [0.61830015]\n",
      "457\n",
      "(10, 1) [0.59225678]\n",
      "458\n",
      "(10, 1) [0.56353917]\n",
      "459\n",
      "(10, 1) [0.54389027]\n",
      "460\n",
      "(10, 1) [0.55249398]\n",
      "461\n",
      "(10, 1) [0.52749678]\n",
      "462\n",
      "(10, 1) [0.5126148]\n",
      "463\n",
      "(10, 1) [0.52924087]\n",
      "464\n",
      "(10, 1) [0.51180104]\n",
      "465\n",
      "(10, 1) [0.54563418]\n",
      "466\n",
      "(10, 1) [0.61609108]\n",
      "467\n",
      "(10, 1) [0.50459245]\n",
      "468\n",
      "(10, 1) [0.5216835]\n",
      "469\n",
      "(10, 1) [0.4875014]\n",
      "470\n",
      "(10, 1) [0.46808507]\n",
      "471\n",
      "(10, 1) [0.51203344]\n",
      "472\n",
      "(10, 1) [0.54016968]\n",
      "473\n",
      "(10, 1) [0.50319732]\n",
      "474\n",
      "(10, 1) [0.49180335]\n",
      "475\n",
      "(10, 1) [0.46494585]\n",
      "476\n",
      "(10, 1) [0.46889902]\n",
      "477\n",
      "(10, 1) [0.48040919]\n",
      "478\n",
      "(10, 1) [0.45808623]\n",
      "479\n",
      "(10, 1) [0.37251477]\n",
      "480\n",
      "(10, 1) [0.32275312]\n",
      "481\n",
      "(10, 1) [0.37216599]\n",
      "482\n",
      "(10, 1) [0.41460302]\n",
      "483\n",
      "(10, 1) [0.41809085]\n",
      "484\n",
      "(10, 1) [0.40681307]\n",
      "485\n",
      "(10, 1) [0.34472731]\n",
      "486\n",
      "(10, 1) [0.34856409]\n",
      "487\n",
      "(10, 1) [0.33182182]\n",
      "488\n",
      "(10, 1) [0.32438084]\n",
      "489\n",
      "(10, 1) [0.37844443]\n",
      "490\n",
      "(10, 1) [0.324846]\n",
      "491\n",
      "(10, 1) [0.371701]\n",
      "492\n",
      "(10, 1) [0.38728055]\n",
      "493\n",
      "(10, 1) [0.38855947]\n",
      "494\n",
      "(10, 1) [0.38309498]\n",
      "495\n",
      "(10, 1) [0.42820607]\n",
      "496\n",
      "(10, 1) [0.45355188]\n",
      "497\n",
      "(10, 1) [0.48715262]\n",
      "498\n",
      "(10, 1) [0.45238933]\n",
      "499\n",
      "(10, 1) [0.39948846]\n",
      "500\n",
      "(10, 1) [0.52668301]\n",
      "501\n",
      "(10, 1) [0.49877917]\n",
      "502\n",
      "(10, 1) [0.46750371]\n",
      "503\n",
      "(10, 1) [0.40216252]\n",
      "504\n",
      "(10, 1) [0.33065928]\n",
      "505\n",
      "(10, 1) [0.324846]\n",
      "506\n",
      "(10, 1) [0.33112426]\n",
      "507\n",
      "(10, 1) [0.33786768]\n",
      "508\n",
      "(10, 1) [0.28403667]\n",
      "509\n",
      "(10, 1) [0.42355534]\n",
      "510\n",
      "(10, 1) [0.45645852]\n",
      "511\n",
      "(10, 1) [0.43994883]\n",
      "512\n",
      "(10, 1) [0.4604115]\n",
      "513\n",
      "(10, 1) [0.44587831]\n",
      "514\n",
      "(10, 1) [0.46831765]\n",
      "515\n",
      "(10, 1) [0.4749447]\n",
      "516\n",
      "(10, 1) [0.43680961]\n",
      "517\n",
      "(10, 1) [0.46203921]\n",
      "518\n",
      "(10, 1) [0.47238702]\n",
      "519\n",
      "(10, 1) [0.43797234]\n",
      "520\n",
      "(10, 1) [0.39274504]\n",
      "521\n",
      "(10, 1) [0.35728401]\n",
      "522\n",
      "(10, 1) [0.43704219]\n",
      "523\n",
      "(10, 1) [0.44029761]\n",
      "524\n",
      "(10, 1) [0.43448433]\n",
      "525\n",
      "(10, 1) [0.42076508]\n",
      "526\n",
      "(10, 1) [0.37751428]\n",
      "527\n",
      "(10, 1) [0.35460995]\n",
      "528\n",
      "(10, 1) [0.37449126]\n",
      "529\n",
      "(10, 1) [0.36879436]\n",
      "530\n",
      "(10, 1) [0.39588426]\n",
      "531\n",
      "(10, 1) [0.40727824]\n",
      "532\n",
      "(10, 1) [0.38100228]\n",
      "533\n",
      "(10, 1) [0.30298802]\n",
      "534\n",
      "(10, 1) [0.27985111]\n",
      "535\n",
      "(10, 1) [0.25497028]\n",
      "536\n",
      "(10, 1) [0.25415652]\n",
      "537\n",
      "(10, 1) [0.2907801]\n",
      "538\n",
      "(10, 1) [0.25334257]\n",
      "539\n",
      "(10, 1) [0.2490408]\n",
      "540\n",
      "(10, 1) [0.22776419]\n",
      "541\n",
      "(10, 1) [0.18137425]\n",
      "542\n",
      "(10, 1) [0.22288105]\n",
      "543\n",
      "(10, 1) [0.22660146]\n",
      "544\n",
      "(10, 1) [0.17009648]\n",
      "545\n",
      "(10, 1) [0.18509475]\n",
      "546\n",
      "(10, 1) [0.16951512]\n",
      "547\n",
      "(10, 1) [0.22299725]\n",
      "548\n",
      "(10, 1) [0.22915932]\n",
      "549\n",
      "(10, 1) [0.23590274]\n",
      "550\n",
      "(10, 1) [0.26799215]\n",
      "551\n",
      "(10, 1) [0.267062]\n",
      "552\n",
      "(10, 1) [0.28275774]\n",
      "553\n",
      "(10, 1) [0.29647717]\n",
      "554\n",
      "(10, 1) [0.28798984]\n",
      "555\n",
      "(10, 1) [0.2886874]\n",
      "556\n",
      "(10, 1) [0.31891634]\n",
      "557\n",
      "(10, 1) [0.35658644]\n",
      "558\n",
      "(10, 1) [0.37309613]\n",
      "559\n",
      "(10, 1) [0.36530636]\n",
      "560\n",
      "(10, 1) [0.3897222]\n",
      "561\n",
      "(10, 1) [0.41262635]\n",
      "562\n",
      "(10, 1) [0.37856063]\n",
      "563\n",
      "(10, 1) [0.39355881]\n",
      "564\n",
      "(10, 1) [0.40681307]\n",
      "565\n",
      "(10, 1) [0.46948038]\n",
      "566\n",
      "(10, 1) [0.51226602]\n",
      "567\n",
      "(10, 1) [0.48006041]\n",
      "568\n",
      "(10, 1) [0.51400993]\n",
      "569\n",
      "(10, 1) [0.48226948]\n",
      "570\n",
      "(10, 1) [0.47006157]\n",
      "571\n",
      "(10, 1) [0.47168928]\n",
      "572\n",
      "(10, 1) [0.50470883]\n",
      "573\n",
      "(10, 1) [0.49715146]\n",
      "574\n",
      "(10, 1) [0.52191608]\n",
      "575\n",
      "(10, 1) [0.50308112]\n",
      "576\n",
      "(10, 1) [0.48959427]\n",
      "577\n",
      "(10, 1) [0.4422741]\n",
      "578\n",
      "(10, 1) [0.44727362]\n",
      "579\n",
      "(10, 1) [0.45297051]\n",
      "580\n",
      "(10, 1) [0.42169523]\n",
      "581\n",
      "(10, 1) [0.43576326]\n",
      "582\n",
      "(10, 1) [0.42983378]\n",
      "583\n",
      "(10, 1) [0.40541794]\n",
      "584\n",
      "(10, 1) [0.41239395]\n",
      "585\n",
      "(10, 1) [0.47192186]\n",
      "586\n",
      "(10, 1) [0.50447625]\n",
      "587\n",
      "(10, 1) [0.47854908]\n",
      "588\n",
      "(10, 1) [0.49331468]\n",
      "589\n",
      "(10, 1) [0.46680614]\n",
      "590\n",
      "(10, 1) [0.44250669]\n",
      "591\n",
      "(10, 1) [0.46541101]\n",
      "592\n",
      "(10, 1) [0.49005926]\n",
      "593\n",
      "(10, 1) [0.49470998]\n",
      "594\n",
      "(10, 1) [0.52796194]\n",
      "595\n",
      "(10, 1) [0.5180793]\n",
      "596\n",
      "(10, 1) [0.54598296]\n",
      "597\n",
      "(10, 1) [0.56784094]\n",
      "598\n",
      "(10, 1) [0.55098247]\n",
      "599\n",
      "(10, 1) [0.56377158]\n",
      "600\n",
      "(10, 1) [0.57911872]\n",
      "601\n",
      "(10, 1) [0.55621439]\n",
      "602\n",
      "(10, 1) [0.5488896]\n",
      "603\n",
      "(10, 1) [0.58516457]\n",
      "(594, 10, 1) (594, 1)\n",
      "(594, 10, 1) (594, 1) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "num_sample = 10\n",
    "training_data_len = int(np.ceil(len(AAPL_Close)*0.8))\n",
    "for i in range(num_sample,training_data_len):\n",
    "    print(i)\n",
    "    x_train.append(AAPL_Close[i-num_sample:i])\n",
    "    y_train.append(AAPL_Close[i])\n",
    "    print(AAPL_Close[i-num_sample:i].shape,AAPL_Close[i])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "print(x_train.shape,y_train.shape)\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "print(x_train.shape,y_train.shape,type(x_train),type(y_train))\n",
    "\n",
    "# # dataset = np.column_stack((data, labels))\n",
    "# dataset = np.column_stack((x_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151, 10, 1) (151, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data = AAPL_Close[training_data_len-num_sample:]\n",
    "# print(test_data)\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in range(num_sample, len(test_data)):\n",
    "    # print(i)\n",
    "    # print(test_data[i-10:i])\n",
    "    x_test.append(test_data[i-num_sample:i])\n",
    "    y_test.append(test_data[i])\n",
    "    \n",
    "# Convert the data to a numpy array\n",
    "x_test,y_test = np.array(x_test),np.array(y_test)\n",
    "# print(x_test)\n",
    "# print(x_test.shape,y_test.shape)\n",
    "# Reshape the data\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1 ))\n",
    "print(x_test.shape,y_test.shape)\n",
    "# print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data  # 输入数据数组，形状为(670, 10, 1)\n",
    "        self.labels = labels  # 对应标签数组，形状为(670, 1)\n",
    "        print(data.shape)\n",
    "        print(type(data))\n",
    "        print(labels.shape)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # self.prepare_data(self):\n",
    "    #     pass\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx], dtype=torch.float)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 10, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(594, 1)\n",
      "(151, 10, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(151, 1)\n"
     ]
    }
   ],
   "source": [
    "# dataset = MyDataset(data, labels)\n",
    "train_dataset = MyDataset(x_train, y_train)\n",
    "val_dataset = MyDataset(x_test, y_test)\n",
    "\n",
    "\n",
    "# print(len(train_dataset))\n",
    "# print(train_dataset[0][0].shape)# 数据   dataset[第几个样本][0 数据  1标签]\n",
    "# print(train_dataset[0][1].shape)# 对应标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1, 1)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1, \n",
    "            hidden_size=128, \n",
    "            batch_first=True, \n",
    "            bidirectional=True)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=256, \n",
    "            hidden_size=64, \n",
    "            batch_first=True, \n",
    "            bidirectional=True)\n",
    "        \n",
    "        # 添加一个全连接层\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64 * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "        # 添加一个全连接层\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        features = self.mlp(x)\n",
    "        print(features.shape)\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        last_lstm_output = lstm_out[:, -1, :]  # 仅获取最后一个时间步的输出\n",
    "        # print('lstm_out',lstm_out.shape)\n",
    "        # print('last_lstm_output',last_lstm_output.shape)\n",
    "        fc1_out = self.fc1(last_lstm_output)\n",
    "        # print('fc1_out',fc1_out.shape)\n",
    "        # fc1_out = fc1_out.squeeze(-1)\n",
    "        # print('fc1_out',fc1_out.shape)\n",
    "        # out = self.fc2(fc1_out)\n",
    "        # print('out',out.shape)\n",
    "        return fc1_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0000003\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (torch.Size([16, 10, 1]), torch.Size([16, 1]))\n",
      "torch.Size([16, 10, 1])\n",
      "torch.Size([16, 10, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 256, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\haokw\\Documents\\GitHub\\VScode\\my_torch\\my_netModel\\sentence\\股票预测torch.ipynb 单元格 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m,(data,label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mid\u001b[39m,(data\u001b[39m.\u001b[39mshape,label\u001b[39m.\u001b[39mshape))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# print(pred.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# pred = pred.squeeze(-1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# print('pred',pred.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# print('label',label.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(pred,label)\n",
      "File \u001b[1;32me:\\conda\\envs\\py39_tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\haokw\\Documents\\GitHub\\VScode\\my_torch\\my_netModel\\sentence\\股票预测torch.ipynb 单元格 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(features\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m lstm_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m last_lstm_output \u001b[39m=\u001b[39m lstm_out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]  \u001b[39m# 仅获取最后一个时间步的输出\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# print('lstm_out',lstm_out.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# print('last_lstm_output',last_lstm_output.shape)\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\py39_tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\conda\\envs\\py39_tf\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:772\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    769\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    770\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 772\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    775\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32me:\\conda\\envs\\py39_tf\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:697\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    693\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[0;32m    694\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    695\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    696\u001b[0m                        ):\n\u001b[1;32m--> 697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[0;32m    698\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    699\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    701\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32me:\\conda\\envs\\py39_tf\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:210\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    208\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    212\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 256, got 1"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    preds = []\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    avg_loss = 0\n",
    "    start_time = time.time()\n",
    "    for id,(data,label) in enumerate(train_loader):\n",
    "        print(id,(data.shape,label.shape))\n",
    "        pred = model(data)\n",
    "        # print(pred.shape)\n",
    "        # pred = pred.squeeze(-1)\n",
    "        # print('pred',pred.shape)\n",
    "        # print('label',label.shape)\n",
    "        loss = criterion(pred,label)\n",
    "        # print(pred,label)\n",
    "        # print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() / len(label)\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "    # model.eval()\n",
    "    # avg_val_loss = 0\n",
    "    \n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for id,(data,label) in enumerate(val_loader):\n",
    "    #         # print(id,(data.shape,label.shape))\n",
    "    #         pred = model(data)\n",
    "    #         # print(pred.shape)\n",
    "    #         # pred = pred.squeeze(-1)\n",
    "    #         # print(pred.shape)\n",
    "    #         # print(label.shape)\n",
    "    #         loss = criterion(pred,label)\n",
    "    #         # print(loss)\n",
    "    #         # optimizer.zero_grad()\n",
    "    #         # loss.backward()\n",
    "    #         # optimizer.step()\n",
    "    #         avg_val_loss += loss.item() / len(label)\n",
    "    #         # break\n",
    "\n",
    "    #         preds.extend(pred.detach().cpu().numpy())\n",
    "    #         # print(np.array(preds).shape)\n",
    "\n",
    "    # preds = np.concatenate(preds, 0)[:,None]\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    elapsed_time = elapsed_time\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"train Epoch {epoch + 1:02d}/{epochs:02d}  lr={lr:.1e}\\t t={elapsed_time:.0f}s  \"\n",
    "        f\"loss={avg_loss:.8f}  val_loss={avg_val_loss:.8f}\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\haokw\\Documents\\GitHub\\VScode\\my_torch\\my_netModel\\sentence\\股票预测torch.ipynb 单元格 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictions \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49minverse_transform(preds)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Plot the data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/haokw/Documents/GitHub/VScode/my_torch/my_netModel/sentence/%E8%82%A1%E7%A5%A8%E9%A2%84%E6%B5%8Btorch.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train \u001b[39m=\u001b[39m AAPL_Close_df[:training_data_len\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32me:\\conda\\envs\\py39_tf\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:544\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Undo the scaling of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \n\u001b[0;32m    532\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[39m    Transformed data.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    542\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 544\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    545\u001b[0m     X, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy, dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    546\u001b[0m )\n\u001b[0;32m    548\u001b[0m X \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_\n\u001b[0;32m    549\u001b[0m X \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_\n",
      "File \u001b[1;32me:\\conda\\envs\\py39_tf\\lib\\site-packages\\sklearn\\utils\\validation.py:938\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    936\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    939\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    940\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    941\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    943\u001b[0m         )\n\u001b[0;32m    945\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(array\u001b[39m.\u001b[39mdtype, \u001b[39m\"\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    946\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    947\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    948\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    949\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "predictions = scaler.inverse_transform(preds)\n",
    "# Plot the data\n",
    "train = AAPL_Close_df[:training_data_len+1]\n",
    "valid = AAPL_Close_df[training_data_len:]\n",
    "\n",
    "valid['Predictions'] = predictions\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price USD ($)', fontsize=18)\n",
    "plt.plot(AAPL_Close_df['Close'],'o')\n",
    "plt.plot(train['Close'])\n",
    "plt.plot(valid[['Close', 'Predictions']])\n",
    "plt.legend(['yuan ','Train', 'Val', 'Predictions'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
