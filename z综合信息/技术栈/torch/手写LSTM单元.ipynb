{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一些常量\n",
    "batch_size, seq_len, input_size, h_size = 100, 1, 6, 32\n",
    "input_6666 = torch.randn(batch_size, seq_len, input_size)  # 随机初始化一个输入序列\n",
    "c_0 = torch.randn(batch_size, h_size)  # 初始值，不会参与训练\n",
    "h_0 = torch.randn(batch_size, h_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用官方 LSTM API\n",
    "lstm_layer = nn.LSTM(input_size, h_size, batch_first=True)  # num_layers默认为1\n",
    "output, (h_n, c_n) = lstm_layer(input_6666, (h_0.unsqueeze(0), c_0.unsqueeze(0)))  # (D*num_layers=1, b, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 32])\n",
      "torch.Size([1, 100, 32])\n",
      "torch.Size([1, 100, 32])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)  # [2,3,5] [b, seq_len, hidden_size]\n",
    "print(h_n.shape)  # [1,2,5] [num_layers, b, hidden_size]\n",
    "print(c_n.shape)  # [1,2,5] [num_layers, b, hidden_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([128, 6])\n",
      "weight_hh_l0 torch.Size([128, 32])\n",
      "bias_ih_l0 torch.Size([128])\n",
      "bias_hh_l0 torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for name, para in lstm_layer.named_parameters():\n",
    "    print(name, para.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(input_6666, initial_states, w_ih, w_hh, b_ih, b_hh):\n",
    "    h_0, c_0 = initial_states  # 初始状态  [b_size, hidden_size]\n",
    "    b_size, seq_len, input_size = input_6666.shape\n",
    "    h_size = h_0.shape[-1]\n",
    "\n",
    "    h_prev, c_prev = h_0, c_0\n",
    "    # 需要将权重w在batch_size维进行扩维并复制，才能和x与h进行相乘\n",
    "    w_ih_batch = w_ih.unsqueeze(0).tile(b_size, 1, 1)  # [4*hidden_size, in_size]->[b_size, ,]\n",
    "    w_hh_batch = w_hh.unsqueeze(0).tile(b_size, 1, 1)  # [4*hidden_size, hidden_size]->[b_size, ,]\n",
    "\n",
    "    output_size = h_size\n",
    "    output = torch.zeros(b_size, seq_len, output_size)  # 初始化一个输出序列\n",
    "    for t in range(seq_len):\n",
    "        x = input_6666[:, t, :]  # 当前时刻的输入向量 [b,in_size]->[b,in_size,1]\n",
    "        w_times_x = torch.bmm(w_ih_batch, x.unsqueeze(-1)).squeeze(-1)   # bmm:含有批量大小的矩阵相乘\n",
    "        # [b, 4*hidden_size, 1]->[b, 4*hidden_size]\n",
    "        # 这一步就是计算了 Wii*xt|Wif*xt|Wig*xt|Wio*xt\n",
    "        w_times_h_prev = torch.bmm(w_hh_batch, h_prev.unsqueeze(-1)).squeeze(-1)\n",
    "        # [b, 4*hidden_size, hidden_size]*[b, hidden_size, 1]->[b,4*hidden_size, 1]->[b, 4*hidden_size]\n",
    "        # 这一步就是计算了 Whi*ht-1|Whf*ht-1|Whg*ht-1|Who*ht-1\n",
    "\n",
    "        # 分别计算输入门(i)、遗忘门(f)、cell门(g)、输出门(o)  维度均为 [b, h_size]\n",
    "        i_t = torch.sigmoid(w_times_x[:, :h_size] + w_times_h_prev[:, :h_size] + b_ih[:h_size] + b_hh[:h_size])  # 取前四分之一\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size] + w_times_h_prev[:, h_size:2*h_size]\n",
    "                            + b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size] + w_times_h_prev[:, 2*h_size:3*h_size]\n",
    "                            + b_ih[2*h_size:3*h_size] + b_hh[2*h_size:3*h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:, 3*h_size:] + w_times_h_prev[:, 3*h_size:]\n",
    "                            + b_ih[3*h_size:] + b_hh[3*h_size:])\n",
    "        c_prev = f_t * c_prev + i_t * g_t\n",
    "        h_prev = o_t * torch.tanh(c_prev)\n",
    "\n",
    "        output[:, t, :] = h_prev\n",
    "\n",
    "    return output, (h_prev.unsqueeze(0), c_prev.unsqueeze(0))  # 官方是三维，在第0维扩一维\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里使用 lstm_layer 中的参数\n",
    "# 加了me表示自己手写的\n",
    "output_me, (h_n_me, c_n_me) = lstm_forward(input_6666, (h_0, c_0), lstm_layer.weight_ih_l0,\n",
    "                                            lstm_layer.weight_hh_l0, lstm_layer.bias_ih_l0, lstm_layer.bias_hh_l0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch API output:\n",
      "tensor([[[ 0.2675, -0.1014,  0.0701,  ..., -0.4516, -0.4680,  0.3969]],\n",
      "\n",
      "        [[-0.2050, -0.1308,  0.3732,  ...,  0.1511, -0.3707,  0.3849]],\n",
      "\n",
      "        [[-0.0065, -0.0293,  0.0346,  ...,  0.4093,  0.1275,  0.0865]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1264,  0.1540, -0.0821,  ...,  0.3826,  0.5380, -0.2471]],\n",
      "\n",
      "        [[-0.0446, -0.1559,  0.0485,  ...,  0.1540, -0.3700, -0.1222]],\n",
      "\n",
      "        [[-0.3258, -0.3508, -0.1058,  ..., -0.1239, -0.3522, -0.3892]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.2675, -0.1014,  0.0701,  ..., -0.4516, -0.4680,  0.3969],\n",
      "         [-0.2050, -0.1308,  0.3732,  ...,  0.1511, -0.3707,  0.3849],\n",
      "         [-0.0065, -0.0293,  0.0346,  ...,  0.4093,  0.1275,  0.0865],\n",
      "         ...,\n",
      "         [-0.1264,  0.1540, -0.0821,  ...,  0.3826,  0.5380, -0.2471],\n",
      "         [-0.0446, -0.1559,  0.0485,  ...,  0.1540, -0.3700, -0.1222],\n",
      "         [-0.3258, -0.3508, -0.1058,  ..., -0.1239, -0.3522, -0.3892]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 1.0530, -0.1580,  0.1740,  ..., -1.1567, -0.9432,  0.5726],\n",
      "         [-0.3666, -0.1976,  0.7355,  ...,  0.2508, -0.9464,  0.6191],\n",
      "         [-0.0116, -0.0663,  0.1070,  ...,  0.6387,  0.2653,  0.1894],\n",
      "         ...,\n",
      "         [-0.2915,  1.0193, -0.2261,  ...,  0.7891,  1.4832, -1.0694],\n",
      "         [-0.2071, -0.3496,  0.1073,  ...,  0.3895, -0.6811, -0.1747],\n",
      "         [-1.1476, -0.6188, -0.1914,  ..., -0.1715, -1.2679, -0.6796]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "\n",
      "lstm_forward function output:\n",
      "tensor([[[ 0.2675, -0.1014,  0.0701,  ..., -0.4516, -0.4680,  0.3969]],\n",
      "\n",
      "        [[-0.2050, -0.1308,  0.3732,  ...,  0.1511, -0.3707,  0.3849]],\n",
      "\n",
      "        [[-0.0065, -0.0293,  0.0346,  ...,  0.4093,  0.1275,  0.0865]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1264,  0.1540, -0.0821,  ...,  0.3826,  0.5380, -0.2471]],\n",
      "\n",
      "        [[-0.0446, -0.1559,  0.0485,  ...,  0.1540, -0.3700, -0.1222]],\n",
      "\n",
      "        [[-0.3258, -0.3508, -0.1058,  ..., -0.1239, -0.3522, -0.3892]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.2675, -0.1014,  0.0701,  ..., -0.4516, -0.4680,  0.3969],\n",
      "         [-0.2050, -0.1308,  0.3732,  ...,  0.1511, -0.3707,  0.3849],\n",
      "         [-0.0065, -0.0293,  0.0346,  ...,  0.4093,  0.1275,  0.0865],\n",
      "         ...,\n",
      "         [-0.1264,  0.1540, -0.0821,  ...,  0.3826,  0.5380, -0.2471],\n",
      "         [-0.0446, -0.1559,  0.0485,  ...,  0.1540, -0.3700, -0.1222],\n",
      "         [-0.3258, -0.3508, -0.1058,  ..., -0.1239, -0.3522, -0.3892]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([[[ 1.0530, -0.1580,  0.1740,  ..., -1.1567, -0.9432,  0.5726],\n",
      "         [-0.3666, -0.1976,  0.7355,  ...,  0.2508, -0.9464,  0.6191],\n",
      "         [-0.0116, -0.0663,  0.1070,  ...,  0.6387,  0.2653,  0.1894],\n",
      "         ...,\n",
      "         [-0.2915,  1.0193, -0.2261,  ...,  0.7891,  1.4832, -1.0694],\n",
      "         [-0.2071, -0.3496,  0.1073,  ...,  0.3895, -0.6811, -0.1747],\n",
      "         [-1.1476, -0.6188, -0.1914,  ..., -0.1715, -1.2679, -0.6796]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "torch.Size([100, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch API output:\")\n",
    "print(output)  # [2,3,5] [b, seq_len, hidden_size]\n",
    "print(h_n)  # [1,2,5] [num_layers, b, hidden_size]\n",
    "print(c_n)  # [1,2,5] [num_layers, b, hidden_size]\n",
    "print(\"\\nlstm_forward function output:\")\n",
    "print(output_me)  # [2,3,5] [b, seq_len, hidden_size]\n",
    "print(h_n_me)  # [1,2,5] [num_layers, b, hidden_size]\n",
    "print(c_n_me)\n",
    "print(output_me.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自己写一个LSTM模型\n",
    "def lstm_forward(input, initial_states, w_ih, w_hh, b_ih, b_hh):\n",
    "    h0, c0 = initial_states #初始状态\n",
    "    bs, T, i_size = input.shape\n",
    "    h_size = w_ih.shape[0] // 4\n",
    "\n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1) #bs,4*hidden_size,i_size\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)#bs,4*size, h_size\n",
    "\n",
    "    output_size = h_size\n",
    "    output = torch.zeros(bs, T, output_size) #输出序列\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:,t, :] #当前时刻的输入向量,[bs, i_size],矩阵相乘后面加一个维度\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1))  #bs, 4*h_size,1\n",
    "        w_times_x = w_times_x.squeeze(-1) #bs, 4*h_size\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))  #bs, 4*h_size,1\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1) #bs, 4*h_size\n",
    "\n",
    "        #分别计算输入们(i)\\遗忘门(f)\\cell门(g)\\输出门(o)\n",
    "        i_t = torch.sigmoid(w_times_x[:, :h_size] + \\\n",
    "            w_times_h_prev[:, :h_size] +b_ih[:h_size] + b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size] + \\\n",
    "            w_times_h_prev[:, h_size:2*h_size] +b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size] + w_times_h_prev[:,2*h_size:3*h_size]\\\n",
    "            +b_ih[2*h_size:3*h_size] + b_hh[2*h_size:3*h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:, 3*h_size:] + \\\n",
    "            w_times_h_prev[:, 3*h_size:] +b_ih[3*h_size:] + b_hh[3*h_size:])\n",
    "        \n",
    "        #然后算记忆元ct,迭代实现\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "        output[:, t, :] = prev_h\n",
    "    return output, (prev_h, prev_c)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_custom, (h_final_custom, c_final_custom) = lstm_forward(input, (h0,c0), \\\n",
    "    lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0, lstm_layer.bias_hh_l0)\n",
    "\n",
    "print(output)\n",
    "print(output_custom)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
