{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 定义要计算的分位数\n",
    "percentiles = [.61, .62]\n",
    "\n",
    "# 使用quantile方法计算分位数\n",
    "result = train_df['Survived'].quantile(percentiles)\n",
    "\n",
    "# 打印结果\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置显示选项的有用代码行，这样我们就可以看到pd数据帧中的所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "train_df = pd.read_csv(\"D:\\\\data\\\\kaggle\\\\titanic\\\\train.csv\")\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Import sklearn classes for model selection, cross validation, and performance evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Import libraries for Hypertuning\n",
    "import optuna\n",
    "\n",
    "# Import libraries for gradient boosting\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
    "from catboost import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn\n",
    "\n",
    "# 自定义颜色值\n",
    "custom_colors = {\n",
    "    \"axes.facecolor\": \"#FAEEE9\",\n",
    "    \"figure.facecolor\": \"#FAEEE9\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\",\n",
    "    \"font.family\": \"arial\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "\n",
    "# 将自定义颜色应用到Seaborn样式\n",
    "sns.set(rc=custom_colors)\n",
    "\n",
    "# 设置显示选项的有用代码行，这样我们就可以看到pd数据帧中的所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Functions\n",
    "def print_sl():\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "\n",
    "def show_na(df, column):\n",
    "    sns.countplot(x='outcome', data=df[df[column].isnull()])\n",
    "    plt.show() \n",
    "\n",
    "\n",
    "def show_data_fea():\n",
    "    # 提取四个数据集的列名字\n",
    "    train_cols = list(train.columns)\n",
    "    test_cols = list(test.columns)\n",
    "    train_orig_cols = list(train_orig.columns)\n",
    "    # 构成一个29*4的数组\n",
    "    all_cols = [train_cols, test_cols, train_orig_cols]\n",
    "    col_lengths = [len(cols) for cols in all_cols]\n",
    "    max_col_length = max(col_lengths)\n",
    "    # 填充空白列名字，使所有子列表长度相等\n",
    "    for cols in all_cols:\n",
    "        while len(cols) < max_col_length:\n",
    "            cols.append(\"\")\n",
    "    # 将四个列表合并成一个二维数组\n",
    "    cols_array = np.array(all_cols).T\n",
    "    # 打印为表格形式，居中显示\n",
    "    for row in cols_array:\n",
    "        print(\"\\t\".join(f\"{col:^{max_col_length}}\" for col in row))\n",
    "    print(f'train shape: {train.shape}')\n",
    "    print(f'test shape: {test.shape}')\n",
    "    print(f'train_orig shape: {train_orig.shape}')\n",
    "    print_sl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:\\\\data\\\\kaggle\\\\playground\\\\train.csv')\n",
    "test = pd.read_csv('D:\\\\data\\\\kaggle\\\\playground\\\\test.csv')\n",
    "sample_submission = pd.read_csv('D:\\\\data\\\\kaggle\\\\playground\\\\sample_submission.csv')\n",
    "train_orig = pd.read_csv('D:\\\\data\\\\kaggle\\\\playground\\\\horse.csv')\n",
    "print('Data Loaded Succesfully!')\n",
    "print_sl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data_fea()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('id',axis=1,inplace=True)\n",
    "test.drop('id',axis=1,inplace=True)\n",
    "show_data_fea()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train shape: {train.shape}')\n",
    "print(f'are there any null values in train: {train.isnull().any().any()}\\n')\n",
    "\n",
    "print(f'test shape: {test.shape}')\n",
    "print(f'are there any null values in test: {test.isnull().any().any()}\\n')\n",
    "\n",
    "print(f'train_orig shape: {train_orig.shape}')\n",
    "print(f'are there any null values in test: {train_orig.isnull().any().any()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['surgery', 'age', 'temp_of_extremities', 'peripheral_pulse', 'mucous_membrane', 'capillary_refill_time',\n",
    "                   'pain', 'peristalsis', 'abdominal_distention', 'nasogastric_tube', 'nasogastric_reflux', 'rectal_exam_feces',\n",
    "                   'abdomen', 'abdomo_appearance', 'surgical_lesion', 'cp_data']\n",
    "print(len(categorical_cols))\n",
    "num_cols = ['hospital_number', 'rectal_temp', 'pulse', 'respiratory_rate', 'nasogastric_reflux_ph', 'packed_cell_volume', 'total_protein',\n",
    "           'abdomo_protein', 'lesion_1', 'lesion_2', 'lesion_3']\n",
    "print(len(num_cols))\n",
    "target = 'outcome'\n",
    "print(f'train shape: {train.shape}')\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count(df: pd.core.frame.DataFrame, col: str, title_name: str='Train') -> None:\n",
    "    # Set background color\n",
    "    \n",
    "    f, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "    s1 = df[col].value_counts()\n",
    "    N = len(s1)\n",
    "\n",
    "    outer_sizes = s1\n",
    "    inner_sizes = s1/N\n",
    "\n",
    "    outer_colors = ['#2878b5', '#9ac9db', '#f8ac8c']  # 替换颜色代码\n",
    "    inner_colors = ['#c82423', '#ff8884', '#ff6905']  # 替换颜色代码\n",
    "\n",
    "    ax[0].pie(\n",
    "        outer_sizes,colors=outer_colors, \n",
    "        labels=s1.index.tolist(), \n",
    "        startangle=90, frame=True, radius=1.3, \n",
    "        explode=([0.05]*(N-1) + [.3]),\n",
    "        wedgeprops={'linewidth' : 1, 'edgecolor' : 'white'}, \n",
    "        textprops={'fontsize': 12, 'weight': 'bold'}\n",
    "    )\n",
    "\n",
    "    textprops = {\n",
    "        'size': 13, \n",
    "        'weight': 'bold', \n",
    "        'color': 'white'\n",
    "    }\n",
    "\n",
    "    ax[0].pie(\n",
    "        inner_sizes, colors=inner_colors,\n",
    "        radius=1, startangle=90,\n",
    "        autopct='%1.f%%', explode=([.1]*(N-1) + [.3]),\n",
    "        pctdistance=0.8, textprops=textprops\n",
    "    )\n",
    "\n",
    "    center_circle = plt.Circle((0,0), .68, color='black', fc='white', linewidth=0)\n",
    "    ax[0].add_artist(center_circle)\n",
    "\n",
    "    x = s1\n",
    "    y = s1.index.tolist()\n",
    "    sns.barplot(\n",
    "        x=x, y=y, ax=ax[1],\n",
    "        palette=['#2878b5', '#9ac9db', '#f8ac8c'],  # 替换颜色代码\n",
    "        orient='horizontal'\n",
    "    )\n",
    "\n",
    "    ax[1].spines['top'].set_visible(False)\n",
    "    ax[1].spines['right'].set_visible(False)\n",
    "    ax[1].tick_params(\n",
    "        axis='x',         \n",
    "        which='both',      \n",
    "        bottom=False,      \n",
    "        labelbottom=False\n",
    "    )\n",
    "\n",
    "    for i, v in enumerate(s1):\n",
    "        ax[1].text(v, i+0.1, str(v), color='black', fontweight='bold', fontsize=12)\n",
    "\n",
    "    plt.setp(ax[1].get_yticklabels(), fontweight=\"bold\")\n",
    "    plt.setp(ax[1].get_xticklabels(), fontweight=\"bold\")\n",
    "    ax[1].set_xlabel(col, fontweight=\"bold\", color='black')\n",
    "    ax[1].set_ylabel('count', fontweight=\"bold\", color='black')\n",
    "\n",
    "    f.suptitle(f'{title_name}', fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_count(train, 'outcome', 'Target Variable(Outcome) Distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, len(categorical_cols)*3))\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    \n",
    "    plt.subplot(len(categorical_cols)//2 + len(categorical_cols) % 2, 2, i+1)\n",
    "    sns.countplot(x=col, hue=\"outcome\", data=train, palette='YlOrRd')\n",
    "    plt.title(f\"{col} countplot by outcome\", fontweight = 'bold')\n",
    "    plt.ylim(0, train[col].value_counts().max() + 10)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, len(num_cols) * 3))\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    # Plotting for outcome\n",
    "    plt.subplot(len(num_cols), 2, i+1)\n",
    "    sns.histplot(x=col, hue=\"outcome\", data=train, bins=30, kde=True, palette='YlOrRd')\n",
    "    plt.title(f\"{col} distribution for outcome\", fontweight=\"bold\")\n",
    "    plt.ylim(0, train[col].value_counts().max() + 10)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/yaaangzhou/playground-s3-e22-eda-modeling/notebook\n",
    "def plot_pair(df_train,num_var,target,plotname):\n",
    "    '''\n",
    "    Funtion to make a pairplot:\n",
    "    df_train: total data\n",
    "    num_var: a list of numeric variable\n",
    "    target: target variable\n",
    "    '''\n",
    "    g = sns.pairplot(data=df_train, x_vars=num_var, y_vars=num_var, hue=target, corner=True,  palette='YlOrRd')\n",
    "    g._legend.set_bbox_to_anchor((0.8, 0.7))\n",
    "    g._legend.set_title(target)\n",
    "    g._legend.loc = 'upper left'\n",
    "    g._legend.get_title().set_fontsize(14)\n",
    "    for item in g._legend.get_texts():\n",
    "        item.set_fontsize(14)\n",
    "\n",
    "    plt.suptitle(plotname, ha='center', fontweight='bold', fontsize=25, y=0.98)\n",
    "    plt.show()\n",
    "\n",
    "plot_pair(train, num_cols, target, plotname = 'Scatter Matrix with Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe\n",
    "df_encoded = train.copy()\n",
    "\n",
    "# Assuming these are your categorical variables, including 'outcome'\n",
    "categorical_vars = ['surgery', 'age', 'temp_of_extremities', 'peripheral_pulse', \n",
    "                    'mucous_membrane', 'capillary_refill_time', 'pain', 'peristalsis', \n",
    "                    'abdominal_distention', 'nasogastric_tube', 'nasogastric_reflux', \n",
    "                    'rectal_exam_feces', 'abdomen', 'abdomo_appearance', 'surgical_lesion', \n",
    "                    'cp_data', 'outcome']\n",
    "\n",
    "# Label encode categorical columns\n",
    "label_encoders = {}\n",
    "for column in categorical_vars:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[column] = le.fit_transform(train[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "def plot_correlation_heatmap(df: pd.core.frame.DataFrame, title_name: str = 'Train correlation') -> None:\n",
    "    excluded_columns = ['id']\n",
    "    columns_without_excluded = [col for col in df.columns if col not in excluded_columns]\n",
    "    corr = df[columns_without_excluded].corr()\n",
    "    \n",
    "    fig, axes = plt.subplots(figsize=(14, 10))\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    sns.heatmap(corr, mask=mask, linewidths=.5, cmap='YlOrBr_r', annot=True, annot_kws={\"size\": 6})\n",
    "    plt.title(title_name)\n",
    "    plt.show()\n",
    "\n",
    "# Plot correlation heatmap for encoded dataframe\n",
    "plot_correlation_heatmap(df_encoded, 'Encoded Dataset Correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, le_cols, ohe_cols):\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    for col in le_cols:\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        \n",
    "    df = pd.get_dummies(df, columns = ohe_cols)\n",
    "    \n",
    "    df[\"pain\"] = df[\"pain\"].replace('slight', 'moderate')\n",
    "    df[\"peristalsis\"] = df[\"peristalsis\"].replace('distend_small', 'normal')\n",
    "    df[\"rectal_exam_feces\"] = df[\"rectal_exam_feces\"].replace('serosanguious', 'absent')\n",
    "    df[\"nasogastric_reflux\"] = df[\"nasogastric_reflux\"].replace('slight', 'none')\n",
    "        \n",
    "    df[\"temp_of_extremities\"] = df[\"temp_of_extremities\"].fillna(\"normal\").map({'cold': 0, 'cool': 1, 'normal': 2, 'warm': 3})\n",
    "    df[\"peripheral_pulse\"] = df[\"peripheral_pulse\"].fillna(\"normal\").map({'absent': 0, 'reduced': 1, 'normal': 2, 'increased': 3})\n",
    "    df[\"capillary_refill_time\"] = df[\"capillary_refill_time\"].fillna(\"3\").map({'less_3_sec': 0, '3': 1, 'more_3_sec': 2})\n",
    "    df[\"pain\"] = df[\"pain\"].fillna(\"depressed\").map({'alert': 0, 'depressed': 1, 'moderate': 2, 'mild_pain': 3, 'severe_pain': 4, 'extreme_pain': 5})\n",
    "    df[\"peristalsis\"] = df[\"peristalsis\"].fillna(\"hypomotile\").map({'hypermotile': 0, 'normal': 1, 'hypomotile': 2, 'absent': 3})\n",
    "    df[\"abdominal_distention\"] = df[\"abdominal_distention\"].fillna(\"none\").map({'none': 0, 'slight': 1, 'moderate': 2, 'severe': 3})\n",
    "    df[\"nasogastric_tube\"] = df[\"nasogastric_tube\"].fillna(\"none\").map({'none': 0, 'slight': 1, 'significant': 2})\n",
    "    df[\"nasogastric_reflux\"] = df[\"nasogastric_reflux\"].fillna(\"none\").map({'less_1_liter': 0, 'none': 1, 'more_1_liter': 2})\n",
    "    df[\"rectal_exam_feces\"] = df[\"rectal_exam_feces\"].fillna(\"absent\").map({'absent': 0, 'decreased': 1, 'normal': 2, 'increased': 3})\n",
    "    df[\"abdomen\"] = df[\"abdomen\"].fillna(\"distend_small\").map({'normal': 0, 'other': 1, 'firm': 2,'distend_small': 3, 'distend_large': 4})\n",
    "    df[\"abdomo_appearance\"] = df[\"abdomo_appearance\"].fillna(\"serosanguious\").map({'clear': 0, 'cloudy': 1, 'serosanguious': 2})\n",
    "    \n",
    "    df.drop('lesion_3',axis=1,inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def features_engineering(df):\n",
    "    df['lesion_2'] = df['lesion_2'].apply(lambda x:1 if x>0 else 0)\n",
    "    data_preprocessed = df.copy()\n",
    "     \n",
    "    data_preprocessed[\"abs_rectal_temp\"] = (data_preprocessed[\"rectal_temp\"] - 37.8).abs()\n",
    "    data_preprocessed.drop(columns=[\"rectal_temp\"])\n",
    "    \n",
    "    return data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_cols = [\"surgery\", \"age\", \"surgical_lesion\", \"cp_data\"]\n",
    "ohe_cols = [\"mucous_membrane\"]\n",
    "\n",
    "train = preprocessing(train, le_cols, ohe_cols)\n",
    "test = preprocessing(test, le_cols, ohe_cols)\n",
    "train_orig = preprocessing(train_orig, le_cols, ohe_cols)\n",
    "\n",
    "#train = encode(train, categorical_cols)\n",
    "#test = encode(test, categorical_cols)\n",
    "#train_orig = encode(train_orig, categorical_cols)\n",
    "\n",
    "total = pd.concat([train, train_orig], ignore_index=True)\n",
    "total.drop_duplicates(inplace=True)\n",
    "\n",
    "total = features_engineering(total)\n",
    "test = features_engineering(test)\n",
    "\n",
    "\n",
    "print(f'train shape: {train.shape}')\n",
    "print(f'are there any null values in train: {train.isnull().any().any()}\\n')\n",
    "\n",
    "print(f'test shape: {test.shape}')\n",
    "print(f'are there any null values in test: {test.isnull().any().any()}\\n')\n",
    "\n",
    "print(f'total shape: {total.shape}')\n",
    "print(f'are there any null values in total: {total.isnull().any().any()}\\n')\n",
    "\n",
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols.remove('lesion_3')\n",
    "num_cols.append('abs_rectal_temp')\n",
    "\n",
    "# Initialize the KNNImputer with the desired number of neighbors\n",
    "imputer = KNNImputer(n_neighbors=12) # 10 is good \n",
    "\n",
    "# Perform KNN imputation\n",
    "df_train_imputed = pd.DataFrame(imputer.fit_transform(total[num_cols]), columns=num_cols)\n",
    "df_test_imputed = pd.DataFrame(imputer.transform(test[num_cols]), columns=num_cols)\n",
    "\n",
    "# Check if there are still missing values in the train and test data sets\n",
    "df_train_null = df_train_imputed[df_train_imputed.isnull().any(axis=1)]\n",
    "df_test_null = df_test_imputed[df_test_imputed.isnull().any(axis=1)]\n",
    "\n",
    "# Display the rows with null values\n",
    "print('No. of records with missing value in Train data set after Imputation : {}'.format(df_train_null.shape[0]))\n",
    "print('No. of records with missing value in Test data set after Imputation : {}'.format(df_test_null.shape[0]))\n",
    "\n",
    "print_sl()\n",
    "\n",
    "# Replace the imputed columns in the train data sets\n",
    "total_2 = total.drop(num_cols, axis=1).reset_index()\n",
    "total_2 = pd.concat([total_2, df_train_imputed], axis=1)\n",
    "\n",
    "# Replace the imputed columns in the test data sets\n",
    "test_2 = test.drop(num_cols, axis=1).reset_index()\n",
    "test_2 = pd.concat([test_2, df_test_imputed], axis=1)\n",
    "\n",
    "# Check the shape of the train and test data set \n",
    "print('Shape of the Total data set : {}'.format(total_2.shape))\n",
    "print('Shape of the Test data set : {}'.format(test_2.shape))\n",
    "\n",
    "total_2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = total_2.drop(columns=[target])\n",
    "y_train = total_2[target].map({'died':0,'euthanized':1,'lived':2})\n",
    "X_test = test_2\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "\n",
    "del train, test, total, test_2, total_2\n",
    "gc.collect()\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    def __init__(self, n_splits=5, test_size=0.2):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "\n",
    "    def split_data(self, X, y, random_state_list):\n",
    "        for random_state in random_state_list:\n",
    "                kf = KFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n",
    "                for train_index, val_index in kf.split(X, y):\n",
    "                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "                    yield X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, n_estimators=1000, device=\"cpu\", random_state=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.device = device\n",
    "        self.random_state = random_state\n",
    "        self.models = self._define_model()\n",
    "        self.models_name = list(self._define_model().keys())\n",
    "        self.len_models = len(self.models)\n",
    "        \n",
    "    def _define_model(self):\n",
    "        \n",
    "        xgb_optuna1 = {\n",
    "            'n_estimators': 500,\n",
    "            'learning_rate': 0.14825592807938784,\n",
    "            'booster': 'gbtree',\n",
    "            'lambda': 8.286104243394034,\n",
    "            'alpha': 3.218706261523848,\n",
    "            'subsample': 0.9641392997798903,\n",
    "            'colsample_bytree': 0.6489144243365093,\n",
    "            'max_depth': 4, \n",
    "            'min_child_weight': 3,\n",
    "            'eta': 1.230361841253566,\n",
    "            'gamma': 0.007588382469327802, \n",
    "            'grow_policy': 'depthwise',\n",
    "            'random_state': self.random_state,\n",
    "        }\n",
    "        \n",
    "        xgb1_params = {\n",
    "            \"n_estimators\": 1000,\n",
    "            \"max_depth\": 3,\n",
    "            \"learning_rate\": 0.55, \n",
    "            \"min_child_weight\": 2,\n",
    "            \"colsample_bytree\": 0.9, \n",
    "            \"objective\": \"multi:softmax\", \n",
    "            \"eval_metric\": \"merror\",\n",
    "            \"random_state\": self.random_state, \n",
    "        }\n",
    "\n",
    "        if self.device == 'gpu':\n",
    "            xgb_params['tree_method'] = 'gpu_hist'\n",
    "            xgb_params['predictor'] = 'gpu_predictor'\n",
    "        \n",
    "        lgb_optuna1 = {\n",
    "            'num_iterations': 200,\n",
    "            'learning_rate': 0.05087818591635374,\n",
    "            'max_depth': 10,\n",
    "            'alpha': 4.34921696876783,\n",
    "            'subsample': 0.512929283477029,\n",
    "            'colsample_bytree': 0.5421760951211009, \n",
    "            'min_child_weight': 4,\n",
    "            'random_state': self.random_state,\n",
    "            'verbose': -1,\n",
    "        }\n",
    "      \n",
    "        cat_optuna1 = {\n",
    "            'iterations': 700,          \n",
    "            'learning_rate': 0.06806932341035855,\n",
    "            'depth': 3,\n",
    "            'l2_leaf_reg': 4.246994639881441,\n",
    "            'bagging_temperature': 0.08262764367292164,\n",
    "            'random_strength': 6.922710769000274, \n",
    "            'border_count': 88,\n",
    "            'random_state': self.random_state,\n",
    "            'verbose': False,\n",
    "        }\n",
    "      \n",
    "        hist_params = {\n",
    "            'l2_regularization': 0.01,\n",
    "            'early_stopping': True,\n",
    "            'learning_rate': 0.01,\n",
    "            'max_iter': self.n_estimators,\n",
    "            'max_depth': 4,\n",
    "            'max_bins': 255,\n",
    "            'min_samples_leaf': 10,\n",
    "            'max_leaf_nodes':10,\n",
    "            'class_weight':'balanced',\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "        models = {\n",
    "            'xgb01': xgb.XGBClassifier(**xgb_optuna1),\n",
    "            'lgb01': lgb.LGBMClassifier(**lgb_optuna1),\n",
    "            #'hgb': HistGradientBoostingClassifier(**hist_params),\n",
    "            'cat01': CatBoostClassifier(**cat_optuna1),\n",
    "            #'xgb': xgb.XGBClassifier(random_state=self.random_state),\n",
    "            'xgb1': xgb.XGBClassifier(**xgb1_params),\n",
    "            #'lgb': lgb.LGBMClassifier(random_state=self.random_state),\n",
    "            #'cat': CatBoostClassifier(random_state=self.random_state),\n",
    "            #'rf': RandomForestClassifier(random_state=self.random_state),\n",
    "        }\n",
    "        \n",
    "        return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaWeights:\n",
    "    def __init__(self, random_state, n_trials=1000):\n",
    "        self.study = None\n",
    "        self.weights = None\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def _objective(self, trial, y_true, y_preds):\n",
    "        # Define the weights for the predictions from each model\n",
    "        weights = [trial.suggest_float(f\"weight{n}\", 1e-12, 2) for n in range(len(y_preds))]\n",
    "\n",
    "        # Calculate the weighted prediction\n",
    "        weighted_pred = np.average(np.array(y_preds), axis=0, weights=weights)\n",
    "        \n",
    "        weighted_pred_labels = np.argmax(weighted_pred, axis=1)\n",
    "        f1_micro_score = f1_score(y_true, weighted_pred_labels, average='micro')\n",
    "        return f1_micro_score\n",
    "\n",
    "    def fit(self, y_true, y_preds):\n",
    "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
    "        pruner = optuna.pruners.HyperbandPruner()\n",
    "        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n",
    "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
    "        self.study.optimize(objective_partial, n_trials=self.n_trials)\n",
    "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
    "\n",
    "    def predict(self, y_preds):\n",
    "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
    "        weighted_pred = np.average(np.array(y_preds), axis=0, weights=self.weights)\n",
    "        return weighted_pred\n",
    "\n",
    "    def fit_predict(self, y_true, y_preds):\n",
    "        self.fit(y_true, y_preds)\n",
    "        return self.predict(y_preds)\n",
    "    \n",
    "    def weights(self):\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "random_state = 42\n",
    "random_state_list = [42] \n",
    "n_estimators = 999 \n",
    "early_stopping_rounds = 333\n",
    "verbose = False\n",
    "device = 'cpu'\n",
    "splitter = Splitter(n_splits=n_splits)\n",
    "\n",
    "# Initialize an array for storing test predictions\n",
    "test_predss = np.zeros((X_test.shape[0], 3))\n",
    "ensemble_f1_score = []\n",
    "weights = []\n",
    "trained_models = {'xgb':[], 'lgb':[], 'cat':[]}\n",
    "    \n",
    "for i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
    "    n = i % n_splits\n",
    "    m = i // n_splits\n",
    "            \n",
    "    # Get a set of Regressor models\n",
    "    classifier = Classifier(n_estimators, device, random_state)\n",
    "    models = classifier.models\n",
    "    \n",
    "    # Initialize lists to store oof and test predictions for each base model\n",
    "    oof_preds = []\n",
    "    test_preds = []\n",
    "    \n",
    "    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n",
    "    for name, model in models.items():\n",
    "        if ('xgb' in name) or ('lgb' in name) or ('cat' in name)  :\n",
    "            model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
    "        else:\n",
    "            model.fit(X_train_, y_train_)\n",
    "            \n",
    "        if name in trained_models.keys():\n",
    "            trained_models[f'{name}'].append(deepcopy(model))\n",
    "        \n",
    "        test_pred = model.predict_proba(X_test)\n",
    "        y_val_pred = model.predict_proba(X_val)\n",
    "\n",
    "        y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "        f1_micro_score = f1_score(y_val, y_val_pred_labels, average='micro')\n",
    "        \n",
    "        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] F1 Micro Score: {f1_micro_score:.5f}')\n",
    "        \n",
    "        oof_preds.append(y_val_pred)\n",
    "        test_preds.append(test_pred)\n",
    "    \n",
    "    # Use Optuna to find the best ensemble weights\n",
    "    optweights = OptunaWeights(random_state=random_state)\n",
    "    y_val_pred = optweights.fit_predict(y_val, oof_preds)\n",
    "    \n",
    "    score = log_loss(y_val, y_val_pred)\n",
    "    y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "    f1_micro_score = f1_score(y_val, y_val_pred_labels, average='micro')\n",
    "    \n",
    "    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] ---------------> F1 Micro Score: {f1_micro_score:.5f}')\n",
    "    print_sl()\n",
    "    \n",
    "    ensemble_f1_score.append(f1_micro_score)\n",
    "    weights.append(optweights.weights)\n",
    "    \n",
    "    # Predict to X_test by the best ensemble weights\n",
    "    _test_preds = optweights.predict(test_preds)\n",
    "    test_predss += _test_preds / (n_splits * len(random_state_list))\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean LogLoss score of the ensemble\n",
    "mean_score = np.mean(ensemble_f1_score)\n",
    "std_score = np.std(ensemble_f1_score)\n",
    "print(f'Ensemble F1 score {mean_score:.5f} ± {std_score:.5f}')\n",
    "\n",
    "# Print the mean and standard deviation of the ensemble weights for each model\n",
    "print('--- Model Weights ---')\n",
    "mean_weights = np.mean(weights, axis=0)\n",
    "std_weights = np.std(weights, axis=0)\n",
    "for name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n",
    "    print(f'{name}: {mean_weight:.5f} ± {std_weight:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': sample_submission['id'], 'outcome': np.argmax(test_predss, axis=1)})\n",
    "submission['outcome'] = submission['outcome'].map({0:'died',1:'euthanized',2:'lived'})\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count(submission, 'outcome', 'Predicted Variable(Outcome) Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_rows\",None)\n",
    "from sklearn import preprocessing\n",
    "import matplotlib \n",
    "matplotlib.style.use('ggplot')\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"D:\\\\data\\\\kaggle\\\\titanic\\\\train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"D:\\\\data\\\\kaggle\\\\titanic\\\\test.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\n",
    "rate_women = sum(women)/len(women)\n",
    "\n",
    "print(\"% of women who survived:\", rate_women)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\n",
    "rate_men = sum(men)/len(men)\n",
    "\n",
    "print(\"% of men who survived:\", rate_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "y = train_data[\"Survived\"]\n",
    "\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "X = pd.get_dummies(train_data[features])\n",
    "X_test = pd.get_dummies(test_data[features])\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=1)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features:', features[0],'\\nlabel:', labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "d2l.plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size,para_cnt):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "     # 使用字符串格式化来打印参数值\n",
    "    para_cnt +=1\n",
    "    for i, param in enumerate(params):\n",
    "        param_values = param.tolist()  # 将tensor转换为Python列表\n",
    "        print(f'Parameter {i + 1}: {param_values}')\n",
    "\n",
    "    print(f'Total number of parameters: {len(params)}')\n",
    "    print(f'Total Cnt of parameters: {para_cnt}')\n",
    "    return para_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 1\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y)  # X和y的小批量损失\n",
    "        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，\n",
    "        # 并以此计算关于[w,b]的梯度\n",
    "        l.sum().backward()\n",
    "        para_cnt = sgd([w, b], lr, batch_size,para_cnt)  # 使用参数的梯度更新参数\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from d2l import torch as d2l\n",
    "\n",
    "d2l.use_svg_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，\n",
    "# 并除以255使得所有像素的数值均在0～1之间\n",
    "trans = transforms.ToTensor()\n",
    "mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=True, transform=trans, download=True)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mnist_train), len(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):  #@save\n",
    "    \"\"\"返回Fashion-MNIST数据集的文本标签\"\"\"\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n",
    "    \"\"\"绘制图像列表\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        if torch.is_tensor(img):\n",
    "            # 图片张量\n",
    "            ax.imshow(img.numpy())\n",
    "        else:\n",
    "            # PIL图片\n",
    "            ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))\n",
    "show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "def get_dataloader_workers():  #@save\n",
    "    \"\"\"使用4个进程来读取数据\"\"\"\n",
    "    return 4\n",
    "\n",
    "train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                             num_workers=get_dataloader_workers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = d2l.Timer()\n",
    "for X, y in train_iter:\n",
    "    continue\n",
    "f'{timer.stop():.2f} sec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None):  #@save\n",
    "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=get_dataloader_workers()),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=get_dataloader_workers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = load_data_fashion_mnist(32, resize=64)\n",
    "for X, y in train_iter:\n",
    "    print(X.shape, X.dtype, y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython import display\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "X.sum(0, keepdim=True), X.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition  # 这里应用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return - torch.log(y_hat[range(len(y_hat)), y])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):  #@save\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):  #@save\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # 将模型设置为评估模式\n",
    "    metric = Accumulator(2)  # 正确预测数、预测总数\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):  #@save\n",
    "    \"\"\"训练模型一个迭代周期（定义见第3章）\"\"\"\n",
    "    # 将模型设置为训练模式\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    # 训练损失总和、训练准确度总和、样本数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # 计算梯度并更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # 使用PyTorch内置的优化器和损失函数\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            # 使用定制的优化器和损失函数\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    # 返回训练损失和训练精度\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator:  #@save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: d2l.set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save\n",
    "    \"\"\"训练模型（定义见第3章）\"\"\"\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "def updater(batch_size):\n",
    "    return d2l.sgd([W, b], lr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch3(net, test_iter, n=6):  #@save\n",
    "    \"\"\"预测标签（定义见第3章）\"\"\"\n",
    "    for X, y in test_iter:\n",
    "        break\n",
    "    trues = d2l.get_fashion_mnist_labels(y)\n",
    "    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))\n",
    "    titles = [true +'\\n' + pred for true, pred in zip(trues, preds)]\n",
    "    d2l.show_images(\n",
    "        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])\n",
    "\n",
    "predict_ch3(net, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = nn.Parameter(torch.randn(\n",
    "    num_inputs, num_hiddens, requires_grad=True) * 0.01)\n",
    "b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\n",
    "W2 = nn.Parameter(torch.randn(\n",
    "    num_hiddens, num_outputs, requires_grad=True) * 0.01)\n",
    "b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n",
    "\n",
    "params = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法\n",
    "    return (H@W2 + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 10, 0.1\n",
    "updater = torch.optim.SGD(params, lr=lr)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.predict_ch3(net, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(784, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, lr, num_epochs = 256, 0.1, 5\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = nn.Parameter(torch.randn(\n",
    "    num_inputs, num_hiddens, requires_grad=True) * 0.01)\n",
    "b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\n",
    "W2 = nn.Parameter(torch.randn(\n",
    "    num_hiddens, num_outputs, requires_grad=True) * 0.01)\n",
    "b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n",
    "\n",
    "params = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法\n",
    "    return (H@W2 + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 10, 0.1\n",
    "para_cnt +=1\n",
    "for i, param in enumerate(params):\n",
    "    param_values = param.tolist()  # 将tensor转换为Python列表\n",
    "    print(f'Parameter {i + 1}: {param_values}')\n",
    "\n",
    "updater = torch.optim.SGD(params, lr=lr)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = 20  # 多项式的最大阶数\n",
    "n_train, n_test = 100, 100  # 训练和测试数据集大小\n",
    "true_w = np.zeros(max_degree)  # 分配大量的空间\n",
    "true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n",
    "\n",
    "features = np.random.normal(size=(n_train + n_test, 1))\n",
    "np.random.shuffle(features)\n",
    "poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\n",
    "for i in range(max_degree):\n",
    "    poly_features[:, i] /= math.gamma(i + 1)  # gamma(n)=(n-1)!\n",
    "# labels的维度:(n_train+n_test,)\n",
    "labels = np.dot(poly_features, true_w)\n",
    "labels += np.random.normal(scale=0.1, size=labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy ndarray转换为tensor\n",
    "true_w, features, poly_features, labels = [torch.tensor(x, dtype=\n",
    "    torch.float32) for x in [true_w, features, poly_features, labels]]\n",
    "\n",
    "features[:2], poly_features[:2, :], labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(net, data_iter, loss):  #@save\n",
    "    \"\"\"评估给定数据集上模型的损失\"\"\"\n",
    "    metric = d2l.Accumulator(2)  # 损失的总和,样本数量\n",
    "    for X, y in data_iter:\n",
    "        out = net(X)\n",
    "        y = y.reshape(out.shape)\n",
    "        l = loss(out, y)\n",
    "        metric.add(l.sum(), l.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_features, test_features, train_labels, test_labels,\n",
    "          num_epochs=400):\n",
    "    loss = nn.MSELoss(reduction='none')\n",
    "    input_shape = train_features.shape[-1]\n",
    "    # 不设置偏置，因为我们已经在多项式中实现了它\n",
    "    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\n",
    "    batch_size = min(10, train_labels.shape[0])\n",
    "    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),\n",
    "                                batch_size)\n",
    "    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),\n",
    "                               batch_size, is_train=False)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',\n",
    "                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],\n",
    "                            legend=['train', 'test'])\n",
    "    for epoch in range(num_epochs):\n",
    "        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\n",
    "        if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),\n",
    "                                     evaluate_loss(net, test_iter, loss)))\n",
    "    print('weight:', net[0].weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!\n",
    "train(poly_features[:n_train, :4], poly_features[n_train:, :4],\n",
    "      labels[:n_train], labels[n_train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从多项式特征中选择前2个维度，即1和x\n",
    "train(poly_features[:n_train, :2], poly_features[n_train:, :2],\n",
    "      labels[:n_train], labels[n_train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    # 在本情况中，所有元素都被丢弃\n",
    "    if dropout == 1:\n",
    "        return np.zeros_like(X)\n",
    "    # 在本情况中，所有元素都被保留\n",
    "    if dropout == 0:\n",
    "        return X\n",
    "    mask = np.random.uniform(0, 1, X.shape) > dropout\n",
    "    return mask.astype(np.float32) * X / (1.0 - dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(16).reshape(2, 8)\n",
    "print(dropout_layer(X, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.normal(0, 1, size=(4,4))\n",
    "print('一个矩阵 \\n',M)\n",
    "for i in range(10):\n",
    "    M = torch.mm(M,torch.normal(0, 1, size=(4, 4)))\n",
    "\n",
    "print('乘以100个矩阵后\\n', M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39_usual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
